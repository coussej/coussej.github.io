<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on coussej</title>
    <link>http://coussej.github.io/post/</link>
    <description>Recent content in Posts on coussej</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 18 Apr 2016 08:41:25 +0000</lastBuildDate>
    <atom:link href="http://coussej.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Building An Open Source Process Historian</title>
      <link>http://coussej.github.io/2016/04/18/Building-An-Open-Source-Process-Historian/</link>
      <pubDate>Mon, 18 Apr 2016 08:41:25 +0000</pubDate>
      
      <guid>http://coussej.github.io/2016/04/18/Building-An-Open-Source-Process-Historian/</guid>
      <description>

&lt;p&gt;This post will explain how and why I built a open source process historian.&lt;/p&gt;

&lt;h3 id=&#34;motivation:5edadc4035440b9febb5d8290a475d8c&#34;&gt;Motivation&lt;/h3&gt;

&lt;p&gt;I have been working in the field of industrial automation, more specifically on MES systems, for nearly four years now. The reason I started exploring the possibilities of an open source alternative for the commercial process historians was to address some of the issues and frustrations I have with them.&lt;/p&gt;

&lt;p&gt;One of those frustrations is that choosing a historian from one of the popular industrial vendors implies a serious vendor lock-in from the very beginning. For starting, they&amp;rsquo;re Windows only, meaning you are already tied to the Mircosoft ecosystem. Of course, Windows is not necessarily bad, but sometimes it is just not the right choice in the server space (even Microsoft itself is starting to realize this, see &lt;a href=&#34;http://blogs.microsoft.com/blog/2016/03/07/announcing-sql-server-on-linux/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://blogs.windows.com/buildingapps/2016/03/30/run-bash-on-ubuntu-on-windows/&#34;&gt;here&lt;/a&gt;). An ideal solution would be platform independent. Also, commercial historians usually integrate well with the respective visualization or MES solution of the same vendor, but not with other external software. This limits you to buying from the same vendor, even if a component from a different vendor is more fit-for-purpose. A good historian (and in general, any other software component) should have a simple, open API for other software to interface with.&lt;/p&gt;

&lt;p&gt;Another issue is the visualization layer. Most of the solutions I&amp;rsquo;ve worked with are not very user-friendly, and require additional client software or specific Java versions to run their applet in a very outdated version of Internet Explorer. Ideally, the visualization should be both easy to work with, and require no dependencies on the client side.&lt;/p&gt;

&lt;h3 id=&#34;approach:5edadc4035440b9febb5d8290a475d8c&#34;&gt;Approach&lt;/h3&gt;

&lt;p&gt;When you go to the Wikipedia page for &lt;a href=&#34;https://en.wikipedia.org/wiki/Operational_historian&#34;&gt;operational historian&lt;/a&gt;, the first sentence summarizes its main function:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Operational historian refers to a database software application that logs or historizes time-based process data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;But we already knew that. What&amp;rsquo;s interesting however, is that when you look closely at that summary, the only thing that makes it specific to industrial automation is the word &amp;ldquo;process&amp;rdquo;. So I dropped the &amp;ldquo;process&amp;rdquo; part and started looking for software that could fill in the functionality described by the other 14 words. You probably already guessed it, but people outside of the industrial world are also collecting and storing time-series data. Their interests are more in the fields of server monitoring and application analytics, but the principle is essentially the same. Even better, the scale they work at is even larger than what is typically required for most industrial companies. I&amp;rsquo;m talking tens of thousands of writes per second, which should be more than enough for competing with the current standards.&lt;/p&gt;

&lt;h4 id=&#34;storing-the-time-series-data:5edadc4035440b9febb5d8290a475d8c&#34;&gt;Storing the time-series data&lt;/h4&gt;

&lt;p&gt;One of those time-series databases is &lt;a href=&#34;https://influxdata.com/&#34;&gt;InfluxDB&lt;/a&gt;. The company behind it, InfluxData, is a YCombinator backed startup that has managed to produce a very mature product in a relatively short time, and it is already being used at major companies like ebay and Google. The database is extremely easy to set up, requires no external dependencies, has a SQL like query syntax and is fully open source. Furthermore, reading and writing data is done by means of HTTP requests, so you can very easily interface with it! All of these features combined make it a good fit for what I want to achieve.&lt;/p&gt;

&lt;p&gt;To demonstrate how easy it is to install InfluxDB, I&amp;rsquo;ll take you through the installation procedure. First, get yourself a standard Ubuntu 14.04 LTS server (or similar). Then, tell the apt-get package manager where to look for InfluxDB by executing these commands:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -sL https://repos.influxdata.com/influxdb.key | sudo apt-key add -
$ source /etc/lsb-release
$ echo &amp;quot;deb https://repos.influxdata.com/${DISTRIB_ID,,} ${DISTRIB_CODENAME} stable&amp;quot; | sudo tee /etc/apt/sources.list.d/influxdb.list
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Update the package list, install InfluxDB and start the service.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install influxdb
$ sudo service influxdb start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s it. It took less than a minute, which is peanuts compared to the time it usually takes to install &lt;em&gt;&amp;ndash; insert industrial software package here &amp;ndash;&lt;/em&gt;. When you browse to &lt;em&gt;&lt;a href=&#34;http://yourServer:8083&#34;&gt;http://yourServer:8083&lt;/a&gt;&lt;/em&gt;, you&amp;rsquo;ll see the influxDB admin page.&lt;/p&gt;

&lt;h4 id=&#34;visualizing-data:5edadc4035440b9febb5d8290a475d8c&#34;&gt;Visualizing data&lt;/h4&gt;

&lt;p&gt;InfluxDB has its own visualization tool, Chronograf, but after trying it I found that it currently lacks some features and it is not mature enough yet for a robust solution. Then I found &lt;a href=&#34;http://grafana.org/&#34;&gt;Grafana&lt;/a&gt;, a large open-source project with the single purpose of providing an easy way of visualizing time-series data in a meaningful way. It provides you with a well thought-out web-based interface, different graph types and the possibility to create beautiful dashboards. It&amp;rsquo;s already close to 3.0, and after using it for a while I can only say it feels like a very mature product. Moreover, InfluxDB is supported as a datasource out of the box.&lt;/p&gt;

&lt;p&gt;Installing and starting Grafana (v3.0 beta) also took less than a minute:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ wget https://grafanarel.s3.amazonaws.com/builds/grafana_3.0.0-beta51460725904_amd64.deb
$ sudo apt-get install -y adduser libfontconfig
$ sudo dpkg -i grafana_3.0.0-beta51460725904_amd64.deb
$ sudo service grafana-server start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When you browse to &lt;em&gt;&lt;a href=&#34;http://yourServer:3000&#34;&gt;http://yourServer:3000&lt;/a&gt;&lt;/em&gt;, you should see Grafana running. In the admin section, add InfluxDB as a datasource, and you&amp;rsquo;re good to go.&lt;/p&gt;

&lt;h4 id=&#34;collecting-the-data:5edadc4035440b9febb5d8290a475d8c&#34;&gt;Collecting the data.&lt;/h4&gt;

&lt;p&gt;So, now we have a platform for storing and visualizing time-series data. This brings us to the point where we have to put the word &amp;ldquo;process&amp;rdquo; back in our functional description, because PLC&amp;rsquo;s don&amp;rsquo;t talk HTTP. PLC&amp;rsquo;s talk ProfiNET, Ethernet/IP, &amp;hellip; Most of the time this is solved by installing an OPC server for each brand of PLC you want to talk to. If you have any experience with OPC, you probably know what a pain it is to get OPC working over a network (usually, it means hours of struggling with Windows DCOM settings). Fortunately, the new version of OPC, OPC Unified Access, is a lot easier to work with, and it is platform independent.&lt;/p&gt;

&lt;p&gt;As there was no OPCUA-to-InfluxDB logger readily available, I decided to roll my own. Using an open source implementation of the OPC-UA stack in javascript (&lt;a href=&#34;http://node-opcua.github.io/&#34;&gt;NodeOPCUA&lt;/a&gt;, another fantastic open source effort!), I wrote an application that polls or monitors a number of PLC values on one side, and writes them to InfluxDB on the other side. Also, when InfluxDB is temporarily unavailable, it buffers the data in a local database. You can find the code &lt;a href=&#34;https://github.com/coussej/node-opcua-logger&#34;&gt;here&lt;/a&gt;. To get it up and running you must first install NodeJS, the server-side javascript platform:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl https://raw.githubusercontent.com/creationix/nvm/v0.31.0/install.sh | bash
$ nvm install 5.10.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, download and install the logger:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/coussej/node-opcua-logger.git
$ cd node-opcua-logger
$ npm install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Modify the config.toml file to your need, as specified in the instructions. You&amp;rsquo;ll have to point it to your OPC Server and your InfluxDB instance, and for each value you want to measure you must repeat the [[measurements]] section. Say you want to read the temperature of an equipment called &amp;ldquo;TANK42&amp;rdquo; from a Siemens PLC every 5 seconds, you&amp;rsquo;ll have to add this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[[measurements]]
name               = &amp;quot;temperature&amp;quot;
tags               = { equipment = &amp;quot;TANK42&amp;quot; }
nodeId             = &amp;quot;ns=3;s=PLC_TANKS.db103.16,r&amp;quot;
collectionType     = &amp;quot;polled&amp;quot;
pollRate           = 12     # samples / minute.
deadbandAbsolute   = 0      # Absolute max difference for a value not to be collected
deadbandRelative   = 0.0    # Relative max difference for a value not to be collected
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, just start the logger:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;node logger.js
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If all goes well (and it should!), you will see your data appearing in InfluxDB.&lt;/p&gt;

&lt;h3 id=&#34;result:5edadc4035440b9febb5d8290a475d8c&#34;&gt;Result&lt;/h3&gt;

&lt;p&gt;The company OPC Labs exposes a &lt;a href=&#34;http://www.opclabs.com/resources/product-information/articles/1094-public-demo-opc-xml-da-server-2&#34;&gt;public OPC UA server&lt;/a&gt; with random data for testing purposes. I set up a test environment connected to this OPC server and managed to achieve a few thousand measurements per second (yes, second) without even stressing the systems. This was with both the OPCUA and InfluxDB on a remote machine, so I imagine local speeds being much higher. Using the data collected from this demo server, I created a grafana dashboard to demonstrate some of the possibilities Grafana provides for the visualization:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../../../img/2016/0417_BuildingAnOpenSourceProcessHistorian/grafana.png&#34; alt=&#34;Grafana dashboad&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Not bad, eh?&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m currently testing this on a production system for monitoring some values in a Siemens S7 PLC, using a Simatic NET v12 server. It&amp;rsquo;s been running for more then a month now without a single failure, so it looks very promising.&lt;/p&gt;

&lt;h4 id=&#34;downsides:5edadc4035440b9febb5d8290a475d8c&#34;&gt;Downsides&lt;/h4&gt;

&lt;p&gt;If you&amp;rsquo;re not used to working in a UNIX-based environment, working with the command line might seem a little scary. It is, actually, but when you get the hang of it you&amp;rsquo;ll never want to go back. I love UNIX and Linux for it&amp;rsquo;s &amp;ldquo;Do one thing, and do it well&amp;rdquo; philosophy. However, if you don&amp;rsquo;t want to go there, you can install all of the above on a Windows box as well. Just be aware that my Ubuntu production box running this full stack is using only 400MB of RAM and 2,5 GB of diskspace. You can&amp;rsquo;t get Windows running on that.&lt;/p&gt;

&lt;p&gt;Another possible downside is the file based configuration management, but I&amp;rsquo;m sure we can improve on that in the future.&lt;/p&gt;

&lt;h4 id=&#34;upsides:5edadc4035440b9febb5d8290a475d8c&#34;&gt;Upsides&lt;/h4&gt;

&lt;p&gt;First of all, it&amp;rsquo;s free. Free as in freedom and free as in beer. The first kind of free means you can do whatever you want with it. Every part of the stack is open source and you can modify it to your liking, if that&amp;rsquo;s what you want. Also, each part is focussed on doing one thing: collecting data, storing data and visualizing data. If you want to switch one component for another (for example by using Chronograph instead of Grafana when it&amp;rsquo;s more mature), you are free to do so. The second kind of free (as in beer) means you don&amp;rsquo;t have to pay any money for it. Indeed, no licenses, neither for the OS nor the different software components. You can add as many measurements as you like at no extra cost. This, combined with the very easy installation, should lower the treshold for companies to try it out.&lt;/p&gt;

&lt;p&gt;Another very important one: it&amp;rsquo;s open. If you want to gather data from some obscure system, you can write a small collector for it that pushes the data over HTTP to InfluxDB. You can write that little collector in any programming language you want (or even a shell script) because making an HTTP request can be done from almost anywhere. This allows you to interface with nearly everything, without being stuck in the toolset or programming language your vendor supplies an SDK for.&lt;/p&gt;

&lt;p&gt;Finally, I think it&amp;rsquo;s all pretty slick. The easy installation (just a few commands) results in a reproducible environment. Both InfluxDB and Grafana leverage modern technologies that result in a nice end-user experience. Querying data is fast, dashboards can be easily created and shared within the company, and using the system requires nothing more than a recent browser.&lt;/p&gt;

&lt;h3 id=&#34;conclusion:5edadc4035440b9febb5d8290a475d8c&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;I created this application and this post because I&amp;rsquo;d like to see the industrial software world approach things in a more open fashion. In the last few years, a lot of big companies have moved from closed, proprietary solutions to open, community-driven software. Even Microsoft has recently open-source the .NET framework, created an open development tool (Visual Studio Code) and is planning to bring SQL Server to Linux. I hope the big industrial vendors will follow this trend, because in my opinion, the current way of working is focussed on keeping customers locked-in and is holding back innovation.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;d also like some like-minded people to try this for themselves, making adjustments and adding features as they go. Although my solution works for me, I&amp;rsquo;m sure that there is still plenty of room for improvement!&lt;/p&gt;

&lt;p&gt;If you have any questions, suggestions, or you just want to get more information, don&amp;rsquo;t hesitate to drop me a line!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Handling JSONB in Go Structs</title>
      <link>http://coussej.github.io/2016/02/16/Handling-JSONB-in-Go-Structs/</link>
      <pubDate>Tue, 16 Feb 2016 10:33:11 +0000</pubDate>
      
      <guid>http://coussej.github.io/2016/02/16/Handling-JSONB-in-Go-Structs/</guid>
      <description>

&lt;p&gt;In a &lt;a href=&#34;http://coussej.github.io/2016/01/14/Replacing-EAV-with-JSONB-in-PostgreSQL/&#34;&gt;previous&lt;/a&gt; post I already described how much database design can be simplified by using the PostgreSQL JSONB datatypes for storing entity properties. Here, I&amp;rsquo;ll show how you can map this design directly onto your structs in Go.&lt;/p&gt;

&lt;p&gt;We want to handle this kind of entity in our application:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;{
  id:          1
  name:        &amp;quot;test entity 1&amp;quot;
  description: &amp;quot;a test entity for some guy&#39;s blog&amp;quot;
  properties:  {
    color:        &amp;quot;red&amp;quot;
    lenght:       120
    width:        3.1882420
    hassomething: true
    country:      &amp;quot;Belgium&amp;quot;
  } 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To store this kind of entity, we create the following table in a PostgreSQL database:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TABLE entity (
  id          SERIAL PRIMARY KEY, 
  name        TEXT, 
  description TEXT,
  properties  JSONB
);
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;handling-in-go:6f7814701979176031e8e8603754b668&#34;&gt;Handling in Go&lt;/h4&gt;

&lt;p&gt;In go, wel&amp;rsquo;ll create a struct with the same fields as our database columns:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type Entity struct {
	Id          int         `db:&amp;quot;id&amp;quot;`
	Name        string      `db:&amp;quot;name&amp;quot;`
	Description string      `db:&amp;quot;description&amp;quot;`
	Properties  ???         `db:&amp;quot;properties&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But what type will we give to the Properties field? Turns out that when querying the JSONB column, the &lt;a href=&#34;https://github.com/lib/pq&#34;&gt;lib/pq&lt;/a&gt; driver will return a bytestring. The most convenient way to work with JSONB coming from a database would be in the form of a &lt;em&gt;map[string]interface{}&lt;/em&gt;, not in the form of a JSON object and most certainly not as bytes. Luckely, the Go standard library has 2 built-in interfaces we can implement to create our own database compatible type: &lt;em&gt;sql.Scanner&lt;/em&gt; &amp;amp; &lt;em&gt;driver.Valuer&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;For more info on these interfaces, check out &lt;a href=&#34;http://jmoiron.net/blog/built-in-interfaces&#34;&gt;this&lt;/a&gt; excellent post. In summary, when you have a type that implements these 2 interfaces, you can use that type with the database/sql package.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;First, we create the type for our properties field. Note that if you have different kinds of entities (orders, customers, books, &amp;hellip;), you can simple re-use this type if they have a similar field:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type PropertyMap map[string]interface{}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we implement the interface. Let&amp;rsquo;s start with &lt;em&gt;driver.Valuer&lt;/em&gt;. To satisfy this interface, we must implement the &lt;em&gt;Value&lt;/em&gt; method, which must transform our type to a database driver compatible type. In our case, we&amp;rsquo;ll marshall the map to JSONB data (= []byte):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (p PropertyMap) Value() (driver.Value, error) {
	j, err := json.Marshal(p)
	return j, err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s it. Time for the second interface, &lt;em&gt;sql.Scanner&lt;/em&gt;, which requires us to implement a &lt;em&gt;Scan&lt;/em&gt; method. This method must take the raw data that comes from the database and transform it to our new type. In our case, the database will return JSONB ([]byte) that we must transform to our type (the reverse of what we did with &lt;em&gt;driver.Valuer&lt;/em&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (p *PropertyMap) Scan(src interface{}) error {
	source, ok := src.([]byte)
	if !ok {
		return errors.New(&amp;quot;Type assertion .([]byte) failed.&amp;quot;)
	}

	var i interface{}
	err := json.Unmarshal(source, &amp;amp;i)
	if err != nil {
		return err
	}

	*p, ok = i.(map[string]interface{})
	if !ok {
		return errors.New(&amp;quot;Type assertion .(map[string]interface{}) failed.&amp;quot;)
	}

	return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s it. Now we can use this type as any other type with the database/sql package:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;e := Entity{Id:1}

err = db.QueryRow(&amp;quot;SELECT name, description, properties FROM entity WHERE id = $1&amp;quot;, 
              e.Id).Scan(&amp;amp;e.Name, &amp;amp;e.Description, &amp;amp;e.Properties)
              
fmt.Printf(&amp;quot;%+v\n&amp;quot;, e)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which results in&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{Id:1 Name:test entity 1 Description:a test entity for some guy&#39;s blog Properties:map[color:red width:3.1882420 length:120 country:Belgium hassomething:true]}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Accessing an individual property can then be done as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;width, ok := e.Properties[&amp;quot;width&amp;quot;].(float64)
color, ok := e.Properties[&amp;quot;color&amp;quot;].(string)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you want even more simplicity, I suggest you take a look at the &lt;a href=&#34;https://github.com/jmoiron/sqlx&#34;&gt;sqlx&lt;/a&gt; package, which extends the standard sql package with some very useful features. For example, instead of selecting a number of rows and scanning them row by row into a struct, sqlx allows you to do this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var test []Entity
db.Select(&amp;amp;test, &amp;quot;SELECT * FROM entity&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ah, how minimal.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Replacing EAV with JSONB in PostgreSQL</title>
      <link>http://coussej.github.io/2016/01/14/Replacing-EAV-with-JSONB-in-PostgreSQL/</link>
      <pubDate>Thu, 14 Jan 2016 21:24:05 +0000</pubDate>
      
      <guid>http://coussej.github.io/2016/01/14/Replacing-EAV-with-JSONB-in-PostgreSQL/</guid>
      <description>

&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: JSONB has potential for greatly simplifying schema design without sacrificing query performance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;introduction:419b4933fdd1f3e6f0f417c34732288f&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;It must be one of the oldest use cases in the world of relational databases: you have an entity, and you need to store certain properties of this entity. But, not all entities have the same set of properties, and properties will be added in the future.&lt;/p&gt;

&lt;p&gt;The most naive way to solve this problem would be to create a column in your table for each property, and just fill in the ones that are relevant. Great! Problem solved. Until your table contains millions of records and you need to add a new non-null property.&lt;/p&gt;

&lt;p&gt;Enter &lt;a href=&#34;https://en.wikipedia.org/wiki/Entity-attribute-value_model&#34;&gt;Entity-Attribute-Value&lt;/a&gt;. I&amp;rsquo;ve seen this pattern in almost every database I&amp;rsquo;ve worked with. One table contains the entities, another table contains the names of the properties (attributes) and a third table links the entities with their attributes and holds the value. This gives you the flexibility for having different sets of properties (attributes) for different entities, and also for adding properties on the fly &lt;em&gt;without locking your table for 3 days&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Nonetheless,  I wouldn&amp;rsquo;t be writing this post if there were no downsides to this approach. Selecting one or more entities based on 1 attribute value requires 2 joins: one with the attribute table and one with the value table. Need entities bases on 2 attributes? That&amp;rsquo;s 4 joins! Also, the properties usually are all stored as strings, which results in type casting, both for the result as for the WHERE clause. If you write a lot of ad-hoc queries, this is very tedious.&lt;/p&gt;

&lt;p&gt;Despite these obvious shortcomings, EAV has been used for a long time to solve this kind of problem. It was a necessary evil, and there just was no better alternative. But then PostgreSQL came along with a new feature&amp;hellip;&lt;/p&gt;

&lt;p&gt;Starting from PostgreSQL 9.4, a &lt;a href=&#34;http://www.postgresql.org/docs/9.5/static/datatype-json.html&#34;&gt;JSONB datatype&lt;/a&gt; was added for storing binary JSON data. While storing JSON in this format usualy takes slightly more space and time then plain text JSON, executing operations on it is much faster. Also JSONB supports indexing, making querying it even faster.&lt;/p&gt;

&lt;p&gt;This new data type enables us to replace the tedious EAV pattern by adding a single JSONB column to our entity table, greatly simplifying the database design. But many argue that this must come with a performance cost. That&amp;rsquo;s why I created this benchmark.&lt;/p&gt;

&lt;h3 id=&#34;test-database-setup:419b4933fdd1f3e6f0f417c34732288f&#34;&gt;Test database setup&lt;/h3&gt;

&lt;p&gt;For this comparison, I created a database on a fresh PostgreSQL 9.5 installation on an 80 $ &lt;a href=&#34;https://www.digitalocean.com/&#34;&gt;DigitalOcean&lt;/a&gt; Ubuntu 14.04 box. After tuning some settings in &lt;em&gt;postgresql.conf&lt;/em&gt;, I ran &lt;a href=&#34;https://gist.github.com/coussej/80c385332ce37df6687f&#34;&gt;this&lt;/a&gt; script using psql.&lt;/p&gt;

&lt;p&gt;The following tables were created for representing the data as EAV.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TABLE entity ( 
  id           SERIAL PRIMARY KEY, 
  name         TEXT, 
  description  TEXT
);
CREATE TABLE entity_attribute (
  id          SERIAL PRIMARY KEY, 
  name        TEXT
);
CREATE TABLE entity_attribute_value (
  id                  SERIAL PRIMARY KEY, 
  entity_id           INT    REFERENCES entity(id), 
  entity_attribute_id INT    REFERENCES entity_attribute(id), 
  value               TEXT
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The table below represents the same data, but with the attributes in a JSONB column which I called properties.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TABLE entity_jsonb (
  id          SERIAL PRIMARY KEY, 
  name        TEXT, 
  description TEXT,
  properties  JSONB
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A lot simpler, isn&amp;rsquo;t it?&lt;/p&gt;

&lt;p&gt;Then, I loaded the exact same data for both patterns for a total of 10 million entities in the form of the one below. This way, we have some different data types among the attribute set.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;{
  id:          1
  name:        &amp;quot;Entity1&amp;quot;
  description: &amp;quot;Test entity no. 1&amp;quot;
  properties:  {
    color:        &amp;quot;red&amp;quot;
    lenght:       120
    width:        3.1882420
    hassomething: true
    country:      &amp;quot;Belgium&amp;quot;
  } 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, we now have the same data stored in both formats. Let&amp;rsquo;s start comparing!&lt;/p&gt;

&lt;h3 id=&#34;query-aesthetics:419b4933fdd1f3e6f0f417c34732288f&#34;&gt;Query aesthetics&lt;/h3&gt;

&lt;p&gt;Earlier it was already clear that the design of the database was greatly simplified by using a JSONB column for the properties instead of using a 3 tabes EAV model. But does this also reflect in the queries?&lt;/p&gt;

&lt;p&gt;Updating a single entity property looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- EAV
UPDATE entity_attribute_value 
SET value = &#39;blue&#39; 
WHERE entity_attribute_id = 1 
  AND entity_id = 120;

-- JSONB
UPDATE entity_jsonb 
SET properties = jsonb_set(properties, &#39;{&amp;quot;color&amp;quot;}&#39;, &#39;&amp;quot;blue&amp;quot;&#39;) 
WHERE id = 120;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Admittedly, the latter doesn&amp;rsquo;t look simpler. To update the property in the JSONB object, we have to use the &lt;em&gt;&lt;a href=&#34;http://www.postgresql.org/docs/9.5/static/functions-json.html&#34;&gt;jsonb_set()&lt;/a&gt;&lt;/em&gt; function, and we have to pass our new value as a JSONB object. However, we don&amp;rsquo;t need to know any id&amp;rsquo;s upfront. When you look at the EAV example, you have to know both the entity_id and the entity_attribute_id to perform the update. If you want to update a property in the JSONB column based on the entity name, go ahead, it&amp;rsquo;s all in the same row.&lt;/p&gt;

&lt;p&gt;Now, let&amp;rsquo;s select that entity we just updated, based on its new color:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- EAV
SELECT e.name 
FROM entity e 
  INNER JOIN entity_attribute_value eav ON e.id = eav.entity_id
  INNER JOIN entity_attribute ea ON eav.entity_attribute_id = ea.id
WHERE ea.name = &#39;color&#39; AND eav.value = &#39;blue&#39;;

-- JSONB
SELECT name 
FROM entity_jsonb 
WHERE properties -&amp;gt;&amp;gt; &#39;color&#39; = &#39;blue&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I think we can agree that the second is both shorter (no joins!) and more pleasing to the eye. A clear win for JSONB! Here, we use the JSON -&amp;gt;&amp;gt; operator to get the color as a text value from the JSONB object. There is also a second way to achieve the same result in the JSONB model, using the @&amp;gt; containment operator:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- JSONB 
SELECT name 
FROM entity_jsonb 
WHERE properties @&amp;gt; &#39;{&amp;quot;color&amp;quot;: &amp;quot;blue&amp;quot;}&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a bit more complex: we check if the JSON object in the properties column contains the object on the right of the operator. Less readable, more performant (see later).&lt;/p&gt;

&lt;p&gt;The simplification of using JSONB is even stronger when you need to select multiple properties at once. This is where the JSONB approach really shines: we just select the properties as extra columns in our result set, without the need for joins.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- JSONB 
SELECT name
  , properties -&amp;gt;&amp;gt; &#39;color&#39;
  , properties -&amp;gt;&amp;gt; &#39;country&#39;
FROM entity_jsonb 
WHERE id = 120;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With EAV, you would need 2 joins per property you want to query.&lt;/p&gt;

&lt;p&gt;In my opinion, the queries above show a great simplification in database design. If you want more examples on how to query JSONB data, check out &lt;a href=&#34;http://schinckel.net/2014/05/25/querying-json-in-postgres/&#34;&gt;this&lt;/a&gt; post.&lt;/p&gt;

&lt;p&gt;Now, let&amp;rsquo;s talk performance.&lt;/p&gt;

&lt;h3 id=&#34;performance:419b4933fdd1f3e6f0f417c34732288f&#34;&gt;Performance&lt;/h3&gt;

&lt;p&gt;To compare performance, I used &lt;a href=&#34;http://www.postgresql.org/docs/9.1/static/sql-explain.html&#34;&gt;EXPLAIN ANALYSE&lt;/a&gt; on the queries above to see how long they take. I did each query at least three times, because the first time the query planner needs some more time. At first, I executed the queries without any indexes. This will obviously be in the advantage of JSONB, as the joins required for EAV can&amp;rsquo;t make use of index scans (the foreign key fields aren&amp;rsquo;t indexed). After that, I created an index on the 2 foreign key columns in the EAV value table, and also a &lt;a href=&#34;http://www.postgresql.org/docs/9.1/static/textsearch-indexes.html&#34;&gt;GIN&lt;/a&gt; index on the JSONB column&lt;/p&gt;

&lt;p&gt;For updating the data, this resulted in these execution times (in ms). Note that the scale is logarithmic:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../../../img/2016/0114_ReplacingEAVwithJSONBinPostgreSQL/update.png&#34; alt=&#34;Update results&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here, we see that the JSONB is much (&amp;gt;50000x) faster than EAV when not using any indexes, for the reason mentioned above. When we index the foreing key columns the difference is almost eliminated, but JSONB is still 1.3x faster than EAV. Notice that the index on the JSONB column does not have any effect here, as we don&amp;rsquo;t use the properties column in the criteria.&lt;/p&gt;

&lt;p&gt;For selecting data based on a property value, we get the following results (normal scale):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../../../img/2016/0114_ReplacingEAVwithJSONBinPostgreSQL/select.png&#34; alt=&#34;Update results&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here we can see that JSONB was again faster without indexes for EAV, but when the index is used EAV is the fastest. But then I noticed the times for the JSONB queries were the same, pointing me to the fact that the GIN index is not used. Apparently, when you use a GIN index on the full properties column, it only has effect when using the containment (@&amp;gt;) operator. I added this to the benchmark and it had a huge effect on the timing: only 0.153ms! That&amp;rsquo;s 15000x faster then EAV, and 25000x faster than the &lt;strong&gt;-&amp;gt;&amp;gt;&lt;/strong&gt; operator.&lt;/p&gt;

&lt;p&gt;For me, that&amp;rsquo;s fast enough.&lt;/p&gt;

&lt;h3 id=&#34;table-size:419b4933fdd1f3e6f0f417c34732288f&#34;&gt;Table size&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s compare the sizes of both approaches. In psql we can show the size of all tables and indexes using the &lt;strong&gt;\dti+&lt;/strong&gt; command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;test-# \dti+
                                                     List of relations
                      Name                      | Type  |         Table          |  Size   |
------------------------------------------------+-------+------------------------+---------+
 entity                                         | table |                        | 730 MB  |
 entity_attribute                               | table |                        | 48 kB   |
 entity_attribute_name_idx                      | index | entity_attribute       | 16 kB   |
 entity_attribute_name_key                      | index | entity_attribute       | 16 kB   |
 entity_attribute_pkey                          | index | entity_attribute       | 16 kB   |
 entity_attribute_value                         | table |                        | 2338 MB |
 entity_attribute_value_entity_attribute_id_idx | index | entity_attribute_value | 1071 MB |
 entity_attribute_value_entity_id_idx           | index | entity_attribute_value | 1071 MB |
 entity_attribute_value_pkey                    | index | entity_attribute_value | 1071 MB |
 entity_jsonb                                   | table |                        | 1817 MB |
 entity_jsonb_pkey                              | index | entity_jsonb           | 214 MB  |
 entity_jsonb_properties_idx                    | index | entity_jsonb           | 104 MB  |
 entity_pkey                                    | index | entity                 | 214 MB  |
(13 rows)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For the EAV model, the tables add up to 3068MB and the indexes add up to 3427MB, resulting in a total 6.43GB. On the other hand, the JSONB model uses 1817MB for the table, and 318MB for the indexes, totalling 2,08GB. Thats 3x less. This suprised me a bit, because we store the property names in each JSONB object. But when you think about it, in EAV we store 2 integer foreign keys per attribute value, resulting in 8 bytes of extra data. Also, in EAV all property values are stored as text, while JSONB will use numeric and boolean values internally where possible, resulting in less space.&lt;/p&gt;

&lt;h3 id=&#34;conclusion:419b4933fdd1f3e6f0f417c34732288f&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;In general, I think storing entity properties in JSONB format can greatly simplify your database design and maintenance. If, like me, you do a lot of ad-hoc querying, having everything stored in the same table as the entity is really useful. The fact that it simplifies interacting with your data is already a plus, but the resulting database is also 3x smaller and from my tests it seems that the performance penalties are very limited. In some cases JSONB even performes faster then EAV, which makes it even better.&lt;/p&gt;

&lt;p&gt;However, this benchmark does of course not cover all aspects (like entities with a very large number of properties, a large increase in the number of properties of existing data, &amp;hellip;), so if you have any suggestions on how to improve it, please feel free to leave a comment!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Listening to generic JSON notifications from PostgreSQL in Go</title>
      <link>http://coussej.github.io/2015/09/15/Listening-to-generic-JSON-notifications-from-PostgreSQL-in-Go/</link>
      <pubDate>Tue, 15 Sep 2015 22:23:45 +0000</pubDate>
      
      <guid>http://coussej.github.io/2015/09/15/Listening-to-generic-JSON-notifications-from-PostgreSQL-in-Go/</guid>
      <description>

&lt;p&gt;It&amp;rsquo;s already widely known that PostgreSQL is the leading open source relational database when it comes to features. One of those features is great JSON support, an other is LISTEN/NOTIFY, a nifty pub-sub sytem exclusive to PostgreSQL. When combining these two, you get a good basis for a real-time push notification system. In this post, I will explain how to create a generic trigger function to generate JSON notifications for any table change, and how to listen to them in Go.&lt;/p&gt;

&lt;h2 id=&#34;the-trigger-function:307f08762c85d301a5b8c6b62d39da46&#34;&gt;The trigger function&lt;/h2&gt;

&lt;p&gt;First, we will create the trigger function. The function will notify a channel &lt;em&gt;events&lt;/em&gt; with the table name, the action and the old/new row data, depending on the action. As there are no table specific references, you can use the same trigger on all tables you want notifications from.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE OR REPLACE FUNCTION notify_event() RETURNS TRIGGER AS $$

    DECLARE 
        data json;
        notification json;
    
    BEGIN
    
        -- Convert the old or new row to JSON, based on the kind of action.
        -- Action = DELETE?             -&amp;gt; OLD row
        -- Action = INSERT or UPDATE?   -&amp;gt; NEW row
        IF (TG_OP = &#39;DELETE&#39;) THEN
            data = row_to_json(OLD);
        ELSE
            data = row_to_json(NEW);
        END IF;
        
        -- Contruct the notification as a JSON string.
        notification = json_build_object(
                          &#39;table&#39;,TG_TABLE_NAME,
                          &#39;action&#39;, TG_OP,
                          &#39;data&#39;, data);
        
                        
        -- Execute pg_notify(channel, notification)
        PERFORM pg_notify(&#39;events&#39;,notification::text);
        
        -- Result is ignored since this is an AFTER trigger
        RETURN NULL; 
    END;
    
$$ LANGUAGE plpgsql;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that the trigger function is created, let&amp;rsquo;s create a sample table&amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TABLE products (
  id SERIAL,
  name TEXT,
  quantity FLOAT
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip; and add a trigger to it. As you can see, we add the trigger for all three actions in only one statement:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TRIGGER products_notify_event
AFTER INSERT OR UPDATE OR DELETE ON products
    FOR EACH ROW EXECUTE PROCEDURE notify_event();
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;result:307f08762c85d301a5b8c6b62d39da46&#34;&gt;Result&lt;/h2&gt;

&lt;p&gt;Fire up &lt;em&gt;psql&lt;/em&gt; or any other PostgreSQL client and execute the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;exampledb=# LISTEN events;
LISTEN
exampledb=# INSERT INTO products (name, quantity)
exampledb-# VALUES (&#39;pen&#39;, 10200);
INSERT 0 1
Asynchronous notification &amp;quot;events&amp;quot; with payload &amp;quot;{&amp;quot;table&amp;quot; : &amp;quot;products&amp;quot;, 
  &amp;quot;action&amp;quot; : &amp;quot;INSERT&amp;quot;, &amp;quot;data&amp;quot; : {&amp;quot;id&amp;quot;:1,&amp;quot;name&amp;quot;:&amp;quot;pen&amp;quot;,&amp;quot;quantity&amp;quot;:10200}}&amp;quot; 
  received from server process with PID 799.
exampledb=#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Et voila, you now get notifications for any action on the table.&lt;/p&gt;

&lt;h2 id=&#34;listening-for-the-notifications-in-go:307f08762c85d301a5b8c6b62d39da46&#34;&gt;Listening for the notifications in Go.&lt;/h2&gt;

&lt;p&gt;Receiving notifications in a terminal doesn&amp;rsquo;t have a lot of real-life value, so the next step would be to capture these events in a server application. In Go, this is fairly easy using the &lt;a href=&#34;https://godoc.org/github.com/lib/pq&#34;&gt;pq&lt;/a&gt; package, which already includes functionality for listening to PostgreSQL notifications.&lt;/p&gt;

&lt;p&gt;The app below is based on the &lt;a href=&#34;http://godoc.org/github.com/lib/pq/listen_example&#34;&gt;sample app&lt;/a&gt; in the pqdocs.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
	&amp;quot;bytes&amp;quot;
	&amp;quot;database/sql&amp;quot;
	&amp;quot;encoding/json&amp;quot;
	&amp;quot;fmt&amp;quot;
	&amp;quot;time&amp;quot;
	&amp;quot;github.com/lib/pq&amp;quot;
)

func waitForNotification(l *pq.Listener) {
	for {
		select {
		case n := &amp;lt;-l.Notify:
			fmt.Println(&amp;quot;Received data from channel [&amp;quot;, n.Channel, &amp;quot;] :&amp;quot;)
			// Prepare notification payload for pretty print
			var prettyJSON bytes.Buffer
			err := json.Indent(&amp;amp;prettyJSON, []byte(n.Extra), &amp;quot;&amp;quot;, &amp;quot;\t&amp;quot;)
			if err != nil {
				fmt.Println(&amp;quot;Error processing JSON: &amp;quot;, err)
				return
			}
			fmt.Println(string(prettyJSON.Bytes()))
			return
		case &amp;lt;-time.After(90 * time.Second):
			fmt.Println(&amp;quot;Received no events for 90 seconds, checking connection&amp;quot;)
			go func() {
				l.Ping()
			}()
			return
		}
	}
}

func main() {
	var conninfo string = &amp;quot;dbname=exampledb user=webapp password=webapp&amp;quot;

	_, err := sql.Open(&amp;quot;postgres&amp;quot;, conninfo)
	if err != nil {
		panic(err)
	}

	reportProblem := func(ev pq.ListenerEventType, err error) {
		if err != nil {
			fmt.Println(err.Error())
		}
	}

	listener := pq.NewListener(conninfo, 10*time.Second, time.Minute, reportProblem)
	err = listener.Listen(&amp;quot;events&amp;quot;)
	if err != nil {
		panic(err)
	}

	fmt.Println(&amp;quot;Start monitoring PostgreSQL...&amp;quot;)
	for {
		waitForNotification(listener)
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, run the app and make a change on the products table. You will see the notifications are being handled by the app:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Received data from channel [ events ] :
{
        &amp;quot;table&amp;quot;: &amp;quot;products&amp;quot;,
        &amp;quot;action&amp;quot;: &amp;quot;INSERT&amp;quot;,
        &amp;quot;data&amp;quot;: {
                &amp;quot;id&amp;quot;: 1,
                &amp;quot;name&amp;quot;: &amp;quot;pen&amp;quot;,
                &amp;quot;quantity&amp;quot;: 10200
        }
}  
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;what-s-next:307f08762c85d301a5b8c6b62d39da46&#34;&gt;What&amp;rsquo;s next?&lt;/h2&gt;

&lt;p&gt;Of course, this example app isn&amp;rsquo;t going to be of much use in real life. A better example would be to create a Websocket http handler that notifies any subscribed clients of the changes, so you can forward the database updates in real time to the clients. I&amp;rsquo;ll take this up in a next post.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>